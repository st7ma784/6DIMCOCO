{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import reduce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about finding the difference between loss(x)+loss(y) and loss(x+y).\n",
    "\n",
    "x and y are shape BxB matrices, where B is the batch size.\n",
    "\n",
    "loss is a function that takes a BxB matrix and returns a scalar using CE loss. The labels are always torch.arange(B).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add then loss = 21.46099090576172\n",
      "Add then loss = 361.396728515625\n"
     ]
    }
   ],
   "source": [
    "B=20\n",
    "n=10\n",
    "vector=torch.rand(n,B,100)\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "labels=torch.arange(B)\n",
    "#to get the first set of losses, we create our for loop to create nxnxBxB matrix\n",
    "\n",
    "losses=[a@b.T for a in vector for b in vector]\n",
    "losses=reduce(torch.add, losses)\n",
    "lossOut=loss(losses, labels)\n",
    "print(\"Add then loss = {}\".format(lossOut))\n",
    "\n",
    "\n",
    "losses=reduce(torch.add, [loss(a@b.T,labels) for a in vector for b in vector])\n",
    "print(\"Add then loss = {}\".format(losses))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets demonstrate why we want to do loss(x+y) instead of loss(x)+loss(y) using torch cuda profiler...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-25 11:44:02 279564:279564 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\n",
      "STAGE:2023-05-25 11:44:02 279564:279564 ActivityProfilerController.cpp:300] Completed Stage: Collection\n",
      "STAGE:2023-05-25 11:44:02 279564:279564 output_json.cpp:417] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem    # of Calls  Total KFLOPs  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::unbind        11.96%       1.622ms        24.79%       3.363ms     305.727us       1.022ms         6.73%       3.256ms     296.000us           0 b           0 b            11            --  \n",
      "                aten::select        12.27%       1.665ms        12.83%       1.741ms      15.827us       1.557ms        10.26%       2.234ms      20.309us           0 b           0 b           110            --  \n",
      "            aten::as_strided         1.33%     181.000us         1.33%     181.000us       0.862us       1.365ms         8.99%       1.365ms       6.500us           0 b           0 b           210            --  \n",
      "               aten::numpy_T        16.57%       2.248ms        30.12%       4.087ms      40.870us       1.508ms         9.94%       4.663ms      46.630us           0 b           0 b           100            --  \n",
      "               aten::permute        12.78%       1.734ms        13.55%       1.839ms      18.390us       2.467ms        16.26%       3.155ms      31.550us           0 b           0 b           100            --  \n",
      "                aten::matmul        13.46%       1.826ms        41.87%       5.680ms      56.800us       1.800ms        11.86%       6.251ms      62.510us     156.25 Kb           0 b           100            --  \n",
      "                    aten::mm        28.41%       3.854ms        28.41%       3.854ms      38.540us       3.374ms        22.23%       4.451ms      44.510us     156.25 Kb     156.25 Kb           100      8000.000  \n",
      "          aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us       1.077ms         7.10%       1.077ms       5.385us           0 b           0 b           200            --  \n",
      "                   aten::add         2.31%     314.000us         2.31%     314.000us       3.172us     886.000us         5.84%     886.000us       8.949us     154.69 Kb     154.69 Kb            99        39.600  \n",
      "                    [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us    -310.94 Kb    -310.94 Kb           199            --  \n",
      "    aten::cross_entropy_loss         0.23%      31.000us         0.84%     114.000us     114.000us      26.000us         0.17%     120.000us     120.000us           4 b      -1.56 Kb             1            --  \n",
      "           aten::log_softmax         0.24%      32.000us         0.33%      45.000us      45.000us      25.000us         0.16%      50.000us      50.000us       1.56 Kb           0 b             1            --  \n",
      "                    aten::to         0.01%       1.000us         0.01%       1.000us       1.000us       7.000us         0.05%       7.000us       7.000us           0 b           0 b             1            --  \n",
      "          aten::_log_softmax         0.09%      12.000us         0.09%      12.000us      12.000us      18.000us         0.12%      18.000us      18.000us       1.56 Kb       1.56 Kb             1            --  \n",
      "           aten::nll_loss_nd         0.10%      14.000us         0.28%      38.000us      38.000us      15.000us         0.10%      44.000us      44.000us           4 b           0 b             1            --  \n",
      "              aten::nll_loss         0.13%      17.000us         0.18%      24.000us      24.000us      16.000us         0.11%      29.000us      29.000us           4 b          -4 b             1            --  \n",
      "      aten::nll_loss_forward         0.05%       7.000us         0.05%       7.000us       7.000us      13.000us         0.09%      13.000us      13.000us           8 b           8 b             1            --  \n",
      "       cudaDeviceSynchronize         0.07%       9.000us         0.07%       9.000us       9.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b             1            --  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.567ms\n",
      "Self CUDA time total: 15.176ms\n",
      "\n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem    # of Calls   Total FLOPs  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::unbind         6.92%       1.205ms        14.44%       2.513ms     228.455us     786.000us         4.12%       2.563ms     233.000us           0 b           0 b            11            --  \n",
      "                aten::select         7.38%       1.284ms         7.51%       1.308ms      11.891us       1.253ms         6.56%       1.777ms      16.155us           0 b           0 b           110            --  \n",
      "            aten::as_strided         0.55%      95.000us         0.55%      95.000us       0.452us       1.056ms         5.53%       1.056ms       5.029us           0 b           0 b           210            --  \n",
      "               aten::numpy_T         6.46%       1.124ms        14.66%       2.551ms      25.510us       1.132ms         5.93%       2.995ms      29.950us           0 b           0 b           100            --  \n",
      "               aten::permute         7.79%       1.356ms         8.20%       1.427ms      14.270us       1.331ms         6.97%       1.863ms      18.630us           0 b           0 b           100            --  \n",
      "                aten::matmul         8.51%       1.482ms        26.18%       4.558ms      45.580us       1.440ms         7.54%       4.992ms      49.920us     156.25 Kb           0 b           100            --  \n",
      "                    aten::mm        17.67%       3.076ms        17.67%       3.076ms      30.760us       2.718ms        14.23%       3.552ms      35.520us     156.25 Kb     156.25 Kb           100   8000000.000  \n",
      "          aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us     834.000us         4.37%     834.000us       4.170us           0 b           0 b           200            --  \n",
      "    aten::cross_entropy_loss        13.13%       2.286ms        44.04%       7.666ms      76.660us       1.850ms         9.69%       8.107ms      81.070us      41.02 Kb    -115.62 Kb           100            --  \n",
      "           aten::log_softmax        13.35%       2.324ms        15.93%       2.773ms      27.730us       1.874ms         9.81%       3.212ms      32.120us     156.25 Kb           0 b           100            --  \n",
      "                    aten::to         0.01%       1.000us         0.01%       1.000us       0.010us     426.000us         2.23%     426.000us       4.260us           0 b           0 b           100            --  \n",
      "          aten::_log_softmax         2.57%     448.000us         2.57%     448.000us       4.480us     912.000us         4.78%     912.000us       9.120us     156.25 Kb     156.25 Kb           100            --  \n",
      "           aten::nll_loss_nd         6.54%       1.138ms        14.98%       2.607ms      26.070us       1.140ms         5.97%       3.045ms      30.450us         400 b        -112 b           100            --  \n",
      "              aten::nll_loss         6.76%       1.176ms         8.44%       1.469ms      14.690us       1.162ms         6.08%       1.905ms      19.050us         512 b        -280 b           100            --  \n",
      "      aten::nll_loss_forward         1.68%     293.000us         1.68%     293.000us       2.930us     743.000us         3.89%     743.000us       7.430us         792 b         792 b           100            --  \n",
      "                    [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us    -197.65 Kb    -197.65 Kb           324            --  \n",
      "                   aten::add         0.64%     111.000us         0.64%     111.000us       1.121us     440.000us         2.30%     440.000us       4.444us         396 b         396 b            99        99.000  \n",
      "       cudaDeviceSynchronize         0.05%       8.000us         0.05%       8.000us       8.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b             1            --  \n",
      "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 17.407ms\n",
      "Self CUDA time total: 19.097ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-25 11:44:02 279564:279564 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\n",
      "STAGE:2023-05-25 11:44:02 279564:279564 ActivityProfilerController.cpp:300] Completed Stage: Collection\n",
      "STAGE:2023-05-25 11:44:02 279564:279564 output_json.cpp:417] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda = True,profile_memory=True, record_shapes=True, with_stack=True, with_flops=True, with_modules=True) as prof:\n",
    "    #for i in range(100):\n",
    "    lossOut=loss(reduce(torch.add, [a@b.T for a in vector for b in vector]), labels)\n",
    "print(prof.key_averages())#.table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "prof.export_chrome_trace(\"trace_add_then_loss.json\")\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda = True,profile_memory=True, record_shapes=True, with_stack=True, with_flops=True, with_modules=True) as prof:\n",
    "    #for i in range(100):\n",
    "    losses=reduce(torch.add, [loss(a@b.T,labels) for a in vector for b in vector])\n",
    "print(prof.key_averages())#.table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "prof.export_chrome_trace(\"trace_loss_then_add.json\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting because.... CE loss emphasises sum(log(e(target)/e(sum(all))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
