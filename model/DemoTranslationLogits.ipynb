{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.10 GiB (GPU 0; 15.89 GiB total capacity; 451.13 MiB already allocated; 1.37 GiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(token_weights.shape) #49408,512\u001b[39;00m\n\u001b[1;32m     11\u001b[0m normed_weights\u001b[38;5;241m=\u001b[39mtoken_weights\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnorm(token_weights,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m token_similarity\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormed_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormed_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#plot the similarity matrix\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(token_similarity)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.10 GiB (GPU 0; 15.89 GiB total capacity; 451.13 MiB already allocated; 1.37 GiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clip,_=clip.load(\"ViT-B/32\",device=\"cuda\")\n",
    "token_layer =clip.token_embedding\n",
    "token_weights= token_layer.weight\n",
    "#print(token_weights.shape) #49408,512\n",
    "\n",
    "normed_weights=token_weights/torch.norm(token_weights,dim=1).unsqueeze(1)\n",
    "token_similarity=torch.matmul(normed_weights,normed_weights.T).detach().cpu().numpy()\n",
    "\n",
    "#plot the similarity matrix\n",
    "\n",
    "plt.imshow(token_similarity)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "#we can see that the similarity matrix is very sparse, which means that the tokens are not very similar to each other. This is a good thing, as it means that the tokens are not redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model were using for translation is going to go straight  into a CLIP model. \n",
    "\n",
    "For human readability it's natural to get as far as a tokenized output before applying the CLIP tokenization layers.\n",
    "\n",
    "To do this the long way, we use the equivalence of a onehot@token.weight === embedding(token_idx) to preserve the gradient over the transition. Unfortunately, it's very intensive on VRAM! \n",
    "\n",
    "There must be a better way! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where we enquire of the penultimate layers to the LM to see if the language head has any useful features. \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "class myMarianMTModel(MarianMTModel):\n",
    "     def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Union[Tuple[torch.Tensor], BaseModelOutput]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        #labels is none so we can ignore...\n",
    "        # if labels is not None:\n",
    "        #     if use_cache:\n",
    "        #         logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "        #     use_cache = False\n",
    "        #     if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "        #         decoder_input_ids = shift_tokens_right(\n",
    "        #             labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "        #         )\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
    "        EOT=\n",
    "        return Seq2SeqLMOutput(\n",
    "            model_outputs=outputs[0],\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
