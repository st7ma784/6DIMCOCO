{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-ce/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch    \n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to test whether it's possible to use a pytorch linear layer with the same parameter as an embedding layer. \n",
    "\n",
    "We're hoping ot see that we can instantiate both. Then as the embedding model is trained, the linear layer also moves. and same vise versa. \n",
    "\n",
    "This is a test to see if we can use the same parameter for both an embedding layer and a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:474: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/user/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/callbacks/batch_size_finder.py:161: UserWarning: Field `model.batch_size` and `model.hparams.batch_size` are mutually exclusive! `model.batch_size` will be used as the initial batch size for scaling. If this is not the intended behavior, please remove either one.\n",
      "  rank_zero_warn(\n",
      "/home/user/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory tb_logs/EmbLinearTest0/0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/user/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1000 and 32x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 173\u001b[0m\n\u001b[1;32m    165\u001b[0m model\u001b[39m=\u001b[39mPTLModule()\n\u001b[1;32m    166\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    167\u001b[0m gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    168\u001b[0m max_epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m    169\u001b[0m logger\u001b[39m=\u001b[39mTensorBoardLogger(\u001b[39m\"\u001b[39m\u001b[39mtb_logs\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmbLinearTest\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m0\u001b[39m),version\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m0\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    170\u001b[0m auto_scale_batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinsearch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m auto_lr_find\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 173\u001b[0m trainer\u001b[39m.\u001b[39;49mtune(model)\n\u001b[1;32m    175\u001b[0m trainer\u001b[39m.\u001b[39mfit(model)\n\u001b[1;32m    176\u001b[0m trainer\u001b[39m.\u001b[39mtest(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:983\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs, method)\u001b[0m\n\u001b[1;32m    980\u001b[0m Trainer\u001b[39m.\u001b[39m_log_api_event(\u001b[39m\"\u001b[39m\u001b[39mtune\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    982\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m--> 983\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(\n\u001b[1;32m    984\u001b[0m         model,\n\u001b[1;32m    985\u001b[0m         train_dataloaders,\n\u001b[1;32m    986\u001b[0m         val_dataloaders,\n\u001b[1;32m    987\u001b[0m         dataloaders,\n\u001b[1;32m    988\u001b[0m         datamodule,\n\u001b[1;32m    989\u001b[0m         scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs,\n\u001b[1;32m    990\u001b[0m         lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs,\n\u001b[1;32m    991\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    994\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py:74\u001b[0m, in \u001b[0;36mTuner._tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs, method)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_scale_batch_size, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     72\u001b[0m         scale_batch_size_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_scale_batch_size)\n\u001b[0;32m---> 74\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mscale_batch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_batch_size(\n\u001b[1;32m     75\u001b[0m         model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mscale_batch_size_kwargs\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[39m# Run learning rate finder:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find:\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/tuner/tuning.py:184\u001b[0m, in \u001b[0;36mTuner.scale_batch_size\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, mode, steps_per_trial, init_val, max_trials, batch_arg_name)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m [batch_size_finder] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[1;32m    185\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidate(model, dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1097\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39m\"\u001b[39;49m\u001b[39mon_fit_start\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1098\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_hyperparams()\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   1393\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1394\u001b[0m             fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   1397\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/callbacks/batch_size_finder.py:183\u001b[0m, in \u001b[0;36mBatchSizeFinder.on_fit_start\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_fit_start\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, pl_module: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_batch_size(trainer, pl_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/callbacks/batch_size_finder.py:168\u001b[0m, in \u001b[0;36mBatchSizeFinder.scale_batch_size\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscale_batch_size\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, pl_module: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     new_size \u001b[39m=\u001b[39m scale_batch_size(\n\u001b[1;32m    169\u001b[0m         trainer,\n\u001b[1;32m    170\u001b[0m         pl_module,\n\u001b[1;32m    171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mode,\n\u001b[1;32m    172\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_trial,\n\u001b[1;32m    173\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_val,\n\u001b[1;32m    174\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_trials,\n\u001b[1;32m    175\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_arg_name,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_batch_size \u001b[39m=\u001b[39m new_size\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_early_exit:\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/tuner/batch_size_scaling.py:59\u001b[0m, in \u001b[0;36mscale_batch_size\u001b[0;34m(trainer, model, mode, steps_per_trial, init_val, max_trials, batch_arg_name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     new_size \u001b[39m=\u001b[39m _run_power_scaling(trainer, model, new_size, batch_arg_name, max_trials, params)\n\u001b[1;32m     58\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinsearch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     new_size \u001b[39m=\u001b[39m _run_binary_scaling(trainer, model, new_size, batch_arg_name, max_trials, params)\n\u001b[1;32m     61\u001b[0m garbage_collection_cuda()\n\u001b[1;32m     63\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinished batch size finder, will continue with full run using batch size \u001b[39m\u001b[39m{\u001b[39;00mnew_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/tuner/batch_size_scaling.py:208\u001b[0m, in \u001b[0;36m_run_binary_scaling\u001b[0;34m(trainer, pl_module, new_size, batch_arg_name, max_trials, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m _reset_progress(trainer)\n\u001b[1;32m    206\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39m# run loop\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     _try_loop_run(trainer, params)\n\u001b[1;32m    209\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m count \u001b[39m>\u001b[39m max_trials:\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/tuner/batch_size_scaling.py:333\u001b[0m, in \u001b[0;36m_try_loop_run\u001b[0;34m(trainer, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m loop\u001b[39m.\u001b[39mload_state_dict(deepcopy(params[\u001b[39m\"\u001b[39m\u001b[39mloop_state_dict\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m    332\u001b[0m loop\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:308\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[1;32m    307\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/open-ce/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[12], line 146\u001b[0m, in \u001b[0;36mPTLModule.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    144\u001b[0m one_hot\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mone_hot(batch[\u001b[39m0\u001b[39m],num_classes\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    145\u001b[0m \u001b[39m#Bx1000 \u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m linear\u001b[39m=\u001b[39mone_hot \u001b[39m@self\u001b[39;49m\u001b[39m.\u001b[39;49memb\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mT\u001b[39m#Bx 32  \u001b[39;00m\n\u001b[1;32m    147\u001b[0m success\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mallclose(linear, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(batch[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    148\u001b[0m \u001b[39m#return 1 or 0 for success\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1000 and 32x1000)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from functools import reduce\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "class PTLModule(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                batch_size=16,\n",
    "                learning_rate=0.001,\n",
    "                logitsversion=17,\n",
    "                n=6,\n",
    "                normlogits=False,\n",
    "                logvariance=False,\n",
    "                maskLosses=0,):\n",
    "        super().__init__(\n",
    "           \n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        self.emb=nn.Embedding(1000, 32) #1000 items in the vocab, 32 dimensional embedding\n",
    "        self.linearEmbedding=nn.Linear(1000 , 32 ) # 1000 items in the vocab, 32 dimensional embedding\n",
    "        self.linearEmbedding.weight=nn.Parameter(self.emb.weight.T) \n",
    "        self.layer1 = nn.Linear(32, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        from model.nargsLossCalculation import get_loss_fn \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.batch_size=batch_size\n",
    "        self.learning_rate=learning_rate\n",
    "        self.calculate_loss=get_loss_fn(logitsversion=logitsversion,norm=normlogits,log=logvariance)\n",
    "        self.n=n\n",
    "        self.maskLoss=maskLosses\n",
    "        self.maskloss=torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        #with torch.no_grad():\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = torch.utils.data.TensorDataset(torch.randint(0, 1000, (10000,)))\n",
    "        self.val_dataset = torch.utils.data.TensorDataset(torch.randint(0, 1000, (1000,)))\n",
    "        self.test_dataset = torch.utils.data.TensorDataset(torch.randint(0, 1000, (1000,)))\n",
    "        # self.labels=torch.diag_embed(torch.diag_embed(torch.diag_embed(torch.diag_embed(torch.diag_embed(torch.ones(self.batch_size,dtype=torch.float,device=self.device))))))\n",
    "        # self.labels=torch.nan_to_num(self.labels)\n",
    "\n",
    "        #B,N=self.batch_size,6\n",
    "        #Views=torch.diag_embed(torch.ones(N,dtype=torch.long)*B-1)+1\n",
    "        #Lossmask=torch.sum(reduce(torch.add,list(map(lambda Arr: torch.nn.functional.one_hot(torch.arange(B).view(*Arr),num_classes=B),Views.tolist()))).pow(4),dim=-1)\n",
    "\n",
    "        #self.masks=torch.unique(torch.flatten(Lossmask,0,-1),dim=0,sorted=False)\n",
    "\n",
    "        self.alpha=None\n",
    "        from model.LossCalculation import get_loss_sum\n",
    "        self.meanloss=get_loss_sum(0)\n",
    "       \n",
    "        from model.LossCalculation import get_loss_calc\n",
    "\n",
    "        self.loss=get_loss_calc(reduction='sum',ver=self.maskLoss,mask=torch.ones([1],device=self.device))\n",
    "    def train_dataloader(self,batch_size=32):\n",
    "      \n",
    "        import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "\n",
    "        return dataloader.DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True,num_workers=8,drop_last=True)\n",
    "    def val_dataloader(self,batch_size=32):\n",
    "      \n",
    "        import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "\n",
    "        return dataloader.DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=True,num_workers=8,drop_last=True)\n",
    "    def test_dataloader(self,batch_size=32):\n",
    "        \n",
    "        import torch.utils.data.dataloader as dataloader\n",
    "    \n",
    "    \n",
    "        return dataloader.DataLoader(self.test_dataset,batch_size=self.batch_size,shuffle=True,num_workers=8,drop_last=True)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #batch shape is n\n",
    "        #print(\"batch\",batch)\n",
    "        n=6\n",
    "        x=self.emb(batch[0]) # should be Bxf \n",
    "        nx=[self(x+torch.randn_like(x))]*n # should be Bxf\n",
    "        # labels=self.label[:(im.shape[0]),:(im.shape[0]),:(im.shape[0]),:(im.shape[0]),:(im.shape[0]),:(im.shape[0])].to(self.device,non_blocking=True) \n",
    "\n",
    "        logits=self.calculate_loss(*nx).mul(torch.exp(self.logit_scale))\n",
    "        #print(\"logits\",logits.shape)\n",
    "        #self.log(\"first logit\",logits[0,0,0,0,0,0],enable_graph=False)\n",
    "        #self.log(\"BAD logit\",logits[1,2,3,4,5,0],enable_graph=False)\n",
    "        # The idea is that good logits are 1s,   bad should be -1s... so if logits are coming back as ~6000....\n",
    "        #  Option 1: divide down.\n",
    "        #  Option 2: 1- output...\n",
    "        # option 3: logarithmic functions? \n",
    "        #print(\"logits\",logits.shape)\n",
    "        #print(\"labels\",labels.shape)\n",
    "        labels=torch.diag_embed(torch.diag_embed(torch.diag_embed(torch.diag_embed(torch.diag_embed(torch.ones_like(batch[0],dtype=torch.float))))))\n",
    "        labels=torch.nan_to_num(labels)\n",
    "        #print(\"labels\",labels.shape)\n",
    "        lossim = self.loss(logits, labels,alpha=self.alpha)\n",
    "            \n",
    "        \n",
    "        loss1 = self.loss(self.calculate_loss(*nx).mul(torch.exp(self.logit_scale)).permute(1,2,3,4,5,0), labels,alpha=self.alpha)\n",
    "        loss2 = self.loss(self.calculate_loss(*nx).mul(torch.exp(self.logit_scale)).permute(2,3,4,5,0,1), labels,alpha=self.alpha)\n",
    "        loss3 = self.loss(self.calculate_loss(*nx).mul(torch.exp(self.logit_scale)).permute(3,4,5,0,1,2), labels,alpha=self.alpha)\n",
    "        loss4 = self.loss(self.calculate_loss(*nx).mul(torch.exp(self.logit_scale)).permute(4,5,0,1,2,3), labels,alpha=self.alpha)\n",
    "        loss5 = self.loss(self.calculate_loss(*nx).mul(torch.exp(self.logit_scale)).permute(5,0,1,2,3,4), labels,alpha=self.alpha)\n",
    "        loss=self.meanloss(I=[lossim],T=[loss1,loss2,loss3,loss4,loss5]).mean()\n",
    "        self.log(\"loss\",loss,enable_graph=False)\n",
    "        return  {\"loss\":loss, \"labels\":batch[0], \"embs\":nx[0]}  \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #concatenate outputs,\n",
    "        # \n",
    "        # create linear regression model\n",
    "        # \n",
    "        # fit model\n",
    "        # \n",
    "        # get score\n",
    "        alllabels=torch.cat([x['labels'] for x in outputs],dim=0)\n",
    "        allembs=torch.cat([x['embs'] for x in outputs],dim=0)\n",
    "        # print(\"all labels\",alllabels.shape)\n",
    "        # print(\"all embs\",allembs.shape)\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        reg =LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=0, n_jobs=-1)\n",
    "        reg.fit(allembs.detach().cpu().numpy(), alllabels.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(\"score\",reg.score(allembs.detach().cpu().numpy(), alllabels.detach().cpu().numpy()),on_step=False,on_epoch=True,prog_bar=True,logger=True)\n",
    "        #store score to be used in numpy plot\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        #heck that converting batch to one hot is with 1000 classes is working\n",
    "        one_hot=torch.nn.functional.one_hot(batch[0],num_classes=1000).float()\n",
    "        #Bx1000 \n",
    "        linear=one_hot @self.emb.weight#Bx 32  \n",
    "        success=torch.allclose(linear, self.emb(batch[0]))\n",
    "        #return 1 or 0 for success\n",
    "        \n",
    "        self.log(\"test success\",success,on_step=False,on_epoch=True,prog_bar=True,logger=True)\n",
    "        return {\"success\":1 if success else 0}\n",
    "    def validation_epoch_end(self,outputs):\n",
    "        success=reduce(operator.add,[x['success'] for x in outputs])/len(outputs)\n",
    "        self.log(\"test success\",success,on_step=False,on_epoch=True,prog_bar=True,logger=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [p for p in self.parameters()], lr=self.hparams.learning_rate, eps=10e-8,\n",
    "            #weight_decay=0.1,\n",
    "            #betas=(0.9, 0.95),\n",
    "            )\n",
    "        return [optimizer]\n",
    "\n",
    "model=PTLModule()\n",
    "trainer = Trainer(\n",
    "gpus=1,\n",
    "max_epochs=20,\n",
    "logger=TensorBoardLogger(\"tb_logs\", name=\"EmbLinearTest{}\".format(0),version=f\"{0}\"),\n",
    "auto_scale_batch_size=\"binsearch\",\n",
    "auto_lr_find=True)\n",
    "\n",
    "trainer.tune(model)\n",
    "\n",
    "trainer.fit(model)\n",
    "#trainer.test(model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
